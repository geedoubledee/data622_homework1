---
title: "DATA 622 - Homework 1"
author: "Glen Dale Davis"
date: "2024-03-03"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

## Packages

```{r packages, warning = FALSE, message = FALSE}
library(caret)
library(DataExplorer)
library(knitr)
library(naniar)
library(RColorBrewer)
library(stopwords)
library(tidytext)
library(tidyverse)

```

## Introduction

We load two labeled text datasets of very different sizes, which we will use to train three different models to classify text based on sentiment and emotion.

* In the small dataset, social media messages from Twitter, Facebook, and Instagram have been classified as primarily conveying positive, negative, or neutral sentiment; our response variable for this dataset will be `SENTIMENT`

* In the large dataset, Twitter messages have been classified as primarily conveying one of six emotions: sadness, joy, love, anger, fear, and surprise; our response variable for this dataset will be `EMOTION`

While the only feature in the large dataset is the text of the message itself, the small dataset has additional features related to the time and date the message was sent, as well as the platform used to send it since the small dataset covers more social media platforms than the large dataset. For the sake of simplicity and comparison later, we will only train models on the text features. 

## Exploratory Data Analysis

We look at summaries of the datasets to confirm the numbers of observations in each and whether there are any missing values to address. 

```{r data1}
my_url1 <- "https://raw.githubusercontent.com/geedoubledee/data622_homework1/main/sentiment_analysis.csv"
small_df <- read.csv(my_url1)
cols <- c("YEAR", "MONTH", "DAY", "TIME", "TXT", "SENTIMENT", "PLATFORM")
colnames(small_df) <- cols
char_cols <- colnames(small_df[, sapply(small_df, class) == "character"])
small_df_sub1 <- small_df |>
    select(-all_of(char_cols))
small_df_sub2 <- small_df |>
    select(all_of(char_cols)) |>
    sapply(str_trim) |>
    as.data.frame()
small_df <- small_df_sub1 |>
    bind_cols(small_df_sub2) |>
    rowid_to_column(var = "ID")
remove <- c("total_observations", "memory_usage")
reorder <- c("rows", "complete_rows", "columns", "discrete_columns",
             "continuous_columns", "all_missing_columns",
             "total_missing_values")
introduce <- small_df |>
    introduce() |>
    select(-all_of(remove))
introduce <- introduce[, reorder]
knitr::kable(t(introduce), format = "simple", caption = "A summary introduction to the small dataset.")

```

```{r data2}
my_url2 <- "https://raw.githubusercontent.com/geedoubledee/data622_homework1/main/text_pt_"
x <- seq(1, 9)
y <- rep(".csv", 9)
files <- paste0(x, y)
cols <- c("TXT", "LAB")
large_df <- as.data.frame(matrix(nrow = 0, ncol = 2))
colnames(large_df) <- cols
large_df <- large_df |>
    mutate(TXT = as.character(TXT),
           LAB = as.integer(LAB))
for (f in files){
    new_rows <- read.csv(paste0(my_url2, f))
    colnames(new_rows) <- cols
    large_df <- large_df |>
        bind_rows(new_rows)
}
large_df <- large_df |>
    rowid_to_column(var = "ID")
rm(new_rows)
my_url3 <- "https://raw.githubusercontent.com/geedoubledee/data622_homework1/main/text_label_map.csv"
txt_lab_map <- read.csv(my_url3)
cols <- c("KEY", "EMOTION")
colnames(txt_lab_map) <- cols
large_df <- large_df |>
    left_join(txt_lab_map, by = join_by(LAB == KEY),
              relationship = "many-to-one")
remove <- c("total_observations", "memory_usage")
reorder <- c("rows", "complete_rows", "columns", "discrete_columns",
             "continuous_columns", "all_missing_columns",
             "total_missing_values")
introduce <- large_df |>
    introduce() |>
    select(-all_of(remove))
introduce <- introduce[, reorder]
knitr::kable(t(introduce), format = "simple", caption = "A summary introduction to the large dataset.")

```

There are less than 500 observations in the small dataset and over 400,000 observations in the large dataset. There are no missing values to address in either dataset.

Next we take a look at the distributions of our response variables:

* `SENTIMENT` in the small dataset

* `EMOTION` in the large dataset

```{r plot1, warning = FALSE, message = FALSE}
cur_theme = theme_set(theme_classic())
palette1 <- brewer.pal(8, "Dark2")
palette2 <- brewer.pal(11, "RdYlBu")
cols <- palette2[c(2, 6, 10)]
names(cols) <- c("negative", "neutral", "positive")
fils <- cols
obs = nrow(small_df)
p1 <- small_df |>
    ggplot(aes(x = SENTIMENT)) +
    geom_histogram(aes(color = SENTIMENT, fill = SENTIMENT), stat = "count") +
    geom_text(stat = "count", aes(label = paste0(round(
        after_stat(count) / obs * 100, 1), "%")),
              size = 4, color = "black", nudge_y = 8) + 
    scale_color_manual(values = cols) +
    scale_fill_manual(values = fils) +
    labs(title = "Distribution of SENTIMENT in the Small Dataset",
         y = "COUNT") +
    theme(legend.position = "none")
p1

```

In the small dataset, the most frequent `SENTIMENT` class is neutral, followed by positive, then negative. There are some slight class imbalances here, but since we'll be doing text analysis, our input variables will primarily be categorical, and using the SMOTE algorithm to fix unbalanced classification problems is therefore not recommended. None of the classes is so rare that we should be worried about the predictive power of any of the models we'll be developing.

```{r plot2, warning = FALSE, message = FALSE}
cols <- palette1[1:6]
emotions <- c("sadness", "joy", "love", "anger", "fear", "surprise")
names(cols) <- emotions
fils <- cols
obs <- nrow(large_df)
p2 <- large_df |>
    ggplot(aes(x = EMOTION)) +
    geom_histogram(aes(color = EMOTION, fill = EMOTION), stat = "count") +
    geom_text(stat = "count", aes(label = paste0(round(
        after_stat(count) / obs * 100, 1), "%")),
              size = 4, color = "white", hjust = 1.1, fontface = "bold") + 
    scale_color_manual(values = cols) +
    scale_fill_manual(values = fils) +
    labs(title = "Distribution of EMOTION in the Large Dataset",
         y = "COUNT") +
    coord_flip() +
    theme(legend.position = "none")
p2

```

In the large dataset, we see a worse class imbalance issue. Joy is the most frequent `EMOTION` in the large dataset, and it occurs nearly 10 times as often as the least frequent `EMOTION`: surprise. This will affect the predictive power of some of the models we'll be developing since we again can't use SMOTE to correct the issue, but one of the models is actually well suited for overcoming it. 

Now we are ready to tokenize our data for text analysis, creating the word features our models will use to predict sentiment and emotion.

## Data Preparation

First, we split the text variable in each dataset into its word components, then any punctuation, numbers, or stopwords are removed. Next, we pivot the word variable we've just created into a boolean matrix with all the words recorded in the text as columns. Within said matrix, a value of 1 indicates a word appears in that text and a value of 0 indicates it does not.

```{r tokenization1}
small_df_tokens <- small_df |>
    unnest_tokens(output = WORD, input = TXT, strip_punct = TRUE) |>
    anti_join(stop_words, by = join_by(WORD == word)) |>
    filter(!grepl('[0-9]', WORD)) |>
    pivot_wider(id_cols = ID, names_from = WORD, values_from = WORD,
                values_fn = list(WORD = length), values_fill = list(WORD = 0))
small_df <- small_df |>
    left_join(small_df_tokens, by = join_by(ID))
large_df_tokens <- large_df |>
    unnest_tokens(output = WORD, input = TXT, strip_punct = TRUE) |>
    anti_join(stop_words, by = join_by(WORD == word)) |>
    filter(!grepl('[0-9]', WORD))
words <- unique(large_df_tokens$WORD)
new_cols <- as.data.frame(matrix(data = 0, nrow = nrow(large_df_tokens),
                                 ncol = length(words)))
colnames(new_cols) <- words
large_df_tokens <- large_df_tokens |>
    bind_cols(new_cols)

```

We could go further by combining singular and plural versions of the same noun or the various tenses of the same verb into one token. We could also attempt to correct for misspellings. However, the text features as they are now will be sufficient for training our models.

The tokenization process has resulted in some observations having `NA` values for all recorded words. This is because they were composed entirely of stopwords and/or numbers. So we replace these `NA` values with 0. 

```{r tokenization2}
small_df[is.na(small_df)] <- 0

```

We remove any feature variables other than our word tokens. 

```{r }
remove <- c("ID", "YEAR", "MONTH", "DAY", "TIME", "TXT", "PLATFORM")
small_df <- small_df |>
    select(-all_of(remove))
remove <- c("ID", "TXT", "LAB")
large_df <- large_df |>
    select(-all_of(remove))

```

We split the datasets into a series of training and test sets so that we can later perform k-fold cross-validation when training our models.

```{r }
set.seed(1006)
sample_set <- createDataPartition(y = small_df$SENTIMENT, p = 0.75, list = FALSE)
small_df_train <- small_df[sample_set, ]
small_df_test <- small_df[-sample_set, ]

```

Next, we train our models. 

## Model Development

```{r modeling1}

```

```{r modeling2}

```

## Model Selection

## Final Evaluation

## Conclusion
